{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8931fb18",
   "metadata": {},
   "source": [
    "# üß† CNN Baseline for Diabetic Retinopathy Classification\n",
    "\n",
    "## üìã Experiment Overview\n",
    "- **Model**: Custom CNN built from scratch\n",
    "- **Task**: Binary classification (No DR vs Has DR)\n",
    "- **Dataset**: Kaggle Diabetic Retinopathy (35,126 images)\n",
    "- **Strategy**: Simple CNN architecture as baseline\n",
    "- **Expected AUC**: 0.60-0.70 (baseline for comparison)\n",
    "\n",
    "## üéØ Key Features:\n",
    "1. **Lightweight architecture** (faster training)\n",
    "2. **From-scratch learning** (no transfer learning)\n",
    "3. **Medical-focused design** (appropriate for retinal images)\n",
    "4. **Class imbalance handling** (focal loss + class weights)\n",
    "5. **Efficient for limited resources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 1: IMPORT LIBRARIES\n",
    "# ============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, GlobalAveragePooling2D,\n",
    "    Dense, Dropout, BatchNormalization, Flatten\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")\n",
    "print(f\"üî• TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üéØ GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f91040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 2: FOCAL LOSS IMPLEMENTATION\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: FOCAL LOSS FOR CLASS IMBALANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=0.75):\n",
    "    \"\"\"\n",
    "    Focal Loss for handling severe class imbalance in medical data.\n",
    "    \n",
    "    Args:\n",
    "        gamma: Focusing parameter (higher = more focus on hard examples)\n",
    "        alpha: Weighting factor for minority class\n",
    "    \n",
    "    Returns:\n",
    "        Focal loss function for Keras\n",
    "    \"\"\"\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # Ensure types\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Calculate p_t\n",
    "        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        \n",
    "        # Calculate alpha_t\n",
    "        alpha_factor = tf.ones_like(y_true) * alpha\n",
    "        alpha_t = tf.where(tf.equal(y_true, 1), alpha_factor, 1 - alpha_factor)\n",
    "        \n",
    "        # Calculate focal weight\n",
    "        cross_entropy = -tf.math.log(p_t)\n",
    "        weight = alpha_t * tf.pow((1 - p_t), gamma)\n",
    "        \n",
    "        # Final loss\n",
    "        loss = weight * cross_entropy\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return focal_loss_fixed\n",
    "\n",
    "print(\"‚úÖ Focal loss function defined!\")\n",
    "print(f\"   Gamma (focusing): {2.0}\")\n",
    "print(f\"   Alpha (weighting): {0.75}\")\n",
    "print(\"   üìã This will help CNN focus on hard-to-classify DR cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a3e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 3: DATA LOADING & SETUP\n",
    "# ============================================\n",
    "# For Kaggle - adjust paths as needed\n",
    "KAGGLE_INPUT_PATH = '/kaggle/input/diabetic-retinopathy-resized'\n",
    "LOCAL_PATH = '/Users/muhirwa/Desktop/projects/diabetes_retinopathy/data'\n",
    "\n",
    "# Try Kaggle path first, fallback to local\n",
    "if os.path.exists(KAGGLE_INPUT_PATH):\n",
    "    DATA_PATH = KAGGLE_INPUT_PATH\n",
    "    print(\"üåê Using Kaggle dataset path\")\n",
    "elif os.path.exists(LOCAL_PATH):\n",
    "    DATA_PATH = LOCAL_PATH\n",
    "    print(\"üíª Using local dataset path\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset not found! Please check paths.\")\n",
    "    DATA_PATH = None\n",
    "\n",
    "print(f\"üìÅ Data path: {DATA_PATH}\")\n",
    "\n",
    "# Load labels\n",
    "if DATA_PATH:\n",
    "    labels_path = os.path.join(DATA_PATH, 'trainLabels.csv')\n",
    "    if os.path.exists(labels_path):\n",
    "        labels_df = pd.read_csv(labels_path)\n",
    "        print(f\"üìä Labels loaded! Total images: {len(labels_df)}\")\n",
    "        \n",
    "        # Convert to binary classification\n",
    "        labels_df['binary_label'] = (labels_df['level'] > 0).astype(int)\n",
    "        \n",
    "        print(\"\\nüéØ Binary classification:\")\n",
    "        print(labels_df['binary_label'].value_counts())\n",
    "        print(f\"  No DR (0): {(labels_df['binary_label']==0).sum()} images ({(labels_df['binary_label']==0).sum()/len(labels_df)*100:.1f}%)\")\n",
    "        print(f\"  Has DR (1): {(labels_df['binary_label']==1).sum()} images ({(labels_df['binary_label']==1).sum()/len(labels_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Add file paths\n",
    "        image_dir = os.path.join(DATA_PATH, 'resized_train_cropped', 'resized_train_cropped')\n",
    "        labels_df['filepath'] = labels_df['image'].apply(lambda x: os.path.join(image_dir, f\"{x}.jpeg\"))\n",
    "        \n",
    "        # Use subset for faster baseline training (uncomment if needed)\n",
    "        # labels_df = labels_df.sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "        # print(f\"Using subset of {len(labels_df)} images for faster baseline training\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Labels file not found at {labels_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd07bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 4: TRAIN/VAL/TEST SPLITS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: CREATE TRAIN/VAL/TEST SPLITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use all images for CNN baseline\n",
    "labels_df_sample = labels_df.copy()\n",
    "print(f\"Using all {len(labels_df_sample)} images for CNN baseline\")\n",
    "\n",
    "# Split: 70% train, 15% validation, 15% test\n",
    "train_df, temp_df = train_test_split(\n",
    "    labels_df_sample,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=labels_df_sample['binary_label']  # Maintain class distribution\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_df['binary_label']\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Data split complete:\")\n",
    "print(f\"  Train:      {len(train_df):6d} images ({len(train_df)/len(labels_df_sample)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_df):6d} images ({len(val_df)/len(labels_df_sample)*100:.1f}%)\")\n",
    "print(f\"  Test:       {len(test_df):6d} images ({len(test_df)/len(labels_df_sample)*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution in each split\n",
    "print(\"\\nüìä Class distribution per split:\")\n",
    "for name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    class_counts = df['binary_label'].value_counts()\n",
    "    print(f\"  {name:5s}: No DR = {class_counts[0]:4d} ({class_counts[0]/len(df)*100:.1f}%), Has DR = {class_counts[1]:4d} ({class_counts[1]/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 5: DATA AUGMENTATION & GENERATORS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: DATA AUGMENTATION & GENERATORS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Conservative augmentation for baseline CNN\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,          # Conservative rotation\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.05,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.9, 1.1], # Minimal brightness change\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation and test (no augmentation)\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Convert labels to strings (required by flow_from_dataframe)\n",
    "train_df['binary_label_str'] = train_df['binary_label'].astype(str)\n",
    "val_df['binary_label_str'] = val_df['binary_label'].astype(str)\n",
    "test_df['binary_label_str'] = test_df['binary_label'].astype(str)\n",
    "\n",
    "# Create generators\n",
    "BATCH_SIZE = 32  # Standard batch size\n",
    "IMG_SIZE = (128, 128)  # Smaller size for faster baseline training\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='filepath',\n",
    "    y_col='binary_label_str',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col='filepath',\n",
    "    y_col='binary_label_str',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='filepath',\n",
    "    y_col='binary_label_str',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Generators created successfully!\")\n",
    "print(f\"   Input size: {IMG_SIZE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Train batches: {len(train_generator)}\")\n",
    "print(f\"   Val batches:   {len(val_generator)}\")\n",
    "print(f\"   Test batches:  {len(test_generator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 6: CNN BASELINE ARCHITECTURE\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: BUILDING CNN BASELINE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def create_cnn_baseline(input_shape=(128, 128, 3)):\n",
    "    \"\"\"\n",
    "    Create a simple CNN baseline for diabetic retinopathy classification.\n",
    "    Designed to be fast and effective for medical images.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First Conv Block\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Second Conv Block\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Third Conv Block\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Fourth Conv Block\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Global Average Pooling (better than Flatten)\n",
    "        GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_cnn_baseline(input_shape=(*IMG_SIZE, 3))\n",
    "\n",
    "print(f\"üß† CNN Baseline Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\n‚úÖ Total parameters: {total_params:,}\")\n",
    "print(f\"üìä Much smaller than ResNet50 ({total_params/25000000:.1f}x smaller)\")\n",
    "print(f\"‚ö° Faster training expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9081dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 7: COMPILE MODEL & CALLBACKS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 7: COMPILE MODEL & SETUP CALLBACKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate class weights\n",
    "y_train_labels = train_df['binary_label'].astype(int).values\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_labels),\n",
    "    y=y_train_labels\n",
    ")\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "print(f\"‚öñÔ∏è  Class weights calculated:\")\n",
    "print(f\"   No DR (0): {class_weight_dict[0]:.3f}\")\n",
    "print(f\"   Has DR (1): {class_weight_dict[1]:.3f}\")\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),  # Standard learning rate\n",
    "    loss=focal_loss(gamma=2.0, alpha=0.75),  # Focal loss for imbalance\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model compiled with:\")\n",
    "print(f\"   Optimizer: Adam (LR=0.001)\")\n",
    "print(f\"   Loss: Focal Loss (Œ≥=2.0, Œ±=0.75)\")\n",
    "print(f\"   Metrics: Accuracy, AUC\")\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=10,  # More patience for baseline\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_auc',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        mode='max',\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    ModelCheckpoint(\n",
    "        'best_cnn_baseline.h5',\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\\nüîß Callbacks configured:\")\n",
    "print(f\"   üìä Monitoring: val_auc (maximize)\")\n",
    "print(f\"   ‚èπÔ∏è  Early stopping: 10 epochs patience\")\n",
    "print(f\"   üìâ Learning rate reduction: 5 epochs patience\")\n",
    "print(f\"   üíæ Model checkpoint: Save best AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3383a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 8: TRAIN CNN BASELINE\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 8: TRAINING CNN BASELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "EPOCHS = 30  # Sufficient for baseline convergence\n",
    "\n",
    "print(f\"‚è≥ Training CNN baseline for up to {EPOCHS} epochs...\")\n",
    "print(f\"üìä Learning rate: 0.001\")\n",
    "print(f\"üéØ Goal: Establish baseline performance for comparison\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "\n",
    "# Get training results\n",
    "best_val_auc = max(history.history['val_auc'])\n",
    "final_epoch = len(history.history['loss'])\n",
    "\n",
    "print(f\"üìä Training Summary:\")\n",
    "print(f\"   Epochs completed: {final_epoch}\")\n",
    "print(f\"   Best validation AUC: {best_val_auc:.4f}\")\n",
    "print(f\"   Training time: {training_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ece12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 9: EVALUATE ON TEST SET\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 9: EVALUATING ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get predictions\n",
    "print(\"üîÆ Generating predictions on test set...\")\n",
    "y_test_proba = model.predict(test_generator, verbose=1)\n",
    "y_test_pred = (y_test_proba > 0.5).astype(int).flatten()\n",
    "y_test_true = test_generator.classes\n",
    "\n",
    "# Calculate metrics\n",
    "test_acc = accuracy_score(y_test_true, y_test_pred)\n",
    "test_prec = precision_score(y_test_true, y_test_pred, zero_division=0)\n",
    "test_rec = recall_score(y_test_true, y_test_pred, zero_division=0)\n",
    "test_f1 = f1_score(y_test_true, y_test_pred, zero_division=0)\n",
    "test_auc = roc_auc_score(y_test_true, y_test_proba)\n",
    "\n",
    "print(f\"\\nüìä Test Set Results (threshold=0.5):\")\n",
    "print(f\"   Accuracy:  {test_acc*100:.2f}%\")\n",
    "print(f\"   Precision: {test_prec*100:.2f}%\")\n",
    "print(f\"   Recall:    {test_rec*100:.2f}%\")\n",
    "print(f\"   F1 Score:  {test_f1*100:.2f}%\")\n",
    "print(f\"   ROC AUC:   {test_auc:.4f}\")\n",
    "\n",
    "# Comparison with other models\n",
    "print(f\"\\nüÜö Comparison with other models:\")\n",
    "models_comparison = [\n",
    "    ('Random Forest', 0.551, 60.2),\n",
    "    ('SVM', 0.505, 58.8),\n",
    "    ('VGG16', 0.706, 1.0),\n",
    "    ('CNN Baseline', test_auc, test_rec*100)\n",
    "]\n",
    "\n",
    "for model_name, auc, recall in models_comparison:\n",
    "    print(f\"   {model_name:<15}: AUC {auc:.3f}, Recall {recall:.1f}%\")\n",
    "\n",
    "# Determine performance level\n",
    "if test_auc >= 0.70:\n",
    "    print(\"üéâ CNN Baseline performed excellently!\")\n",
    "elif test_auc >= 0.60:\n",
    "    print(\"üëç CNN Baseline shows good performance!\")\n",
    "elif test_auc >= 0.55:\n",
    "    print(\"üìà CNN Baseline shows moderate performance\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CNN Baseline needs improvement\")\n",
    "\n",
    "print(f\"\\nüéØ Expected baseline performance: AUC 0.60-0.70\")\n",
    "if test_auc >= 0.60:\n",
    "    print(\"‚úÖ Baseline performance target achieved!\")\n",
    "else:\n",
    "    print(\"üìä Consider adjusting architecture or hyperparameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 10: THRESHOLD OPTIMIZATION\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 10: THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find optimal threshold for medical use\n",
    "precision_curve, recall_curve, thresholds = precision_recall_curve(y_test_true, y_test_proba)\n",
    "\n",
    "# Target 60% recall for baseline (lower than transfer learning)\n",
    "target_recall = 0.60\n",
    "recall_diffs = np.abs(recall_curve - target_recall)\n",
    "optimal_idx = np.argmin(recall_diffs)\n",
    "optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
    "\n",
    "print(f\"üéØ Threshold optimization:\")\n",
    "print(f\"   Target recall: {target_recall*100:.0f}% (baseline target)\")\n",
    "print(f\"   Optimal threshold: {optimal_threshold:.3f}\")\n",
    "\n",
    "# Test different thresholds\n",
    "print(f\"\\nüìä Performance at different thresholds:\")\n",
    "print(f\"{'Threshold':<10} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for thresh in [0.1, 0.2, 0.3, 0.4, 0.5, optimal_threshold]:\n",
    "    y_pred_thresh = (y_test_proba > thresh).astype(int).flatten()\n",
    "    acc = accuracy_score(y_test_true, y_pred_thresh)\n",
    "    prec = precision_score(y_test_true, y_pred_thresh, zero_division=0)\n",
    "    rec = recall_score(y_test_true, y_pred_thresh, zero_division=0)\n",
    "    f1 = f1_score(y_test_true, y_pred_thresh, zero_division=0)\n",
    "    \n",
    "    print(f\"{thresh:<10.2f} {acc*100:<10.1f} {prec*100:<10.1f} {rec*100:<10.1f} {f1*100:<10.1f}\")\n",
    "\n",
    "# Optimal threshold results\n",
    "y_pred_optimal = (y_test_proba > optimal_threshold).astype(int).flatten()\n",
    "optimal_acc = accuracy_score(y_test_true, y_pred_optimal)\n",
    "optimal_prec = precision_score(y_test_true, y_pred_optimal, zero_division=0)\n",
    "optimal_rec = recall_score(y_test_true, y_pred_optimal, zero_division=0)\n",
    "optimal_f1 = f1_score(y_test_true, y_pred_optimal, zero_division=0)\n",
    "\n",
    "print(f\"\\nüè• Optimized for baseline use (threshold={optimal_threshold:.3f}):\")\n",
    "print(f\"   Accuracy:  {optimal_acc*100:.2f}%\")\n",
    "print(f\"   Precision: {optimal_prec*100:.2f}%\")\n",
    "print(f\"   Recall:    {optimal_rec*100:.2f}%\")\n",
    "print(f\"   F1 Score:  {optimal_f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 11: VISUALIZATION & ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 11: VISUALIZATION & ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('CNN Baseline Training Progress & Results', fontsize=16)\n",
    "\n",
    "# Loss plot\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss', color='blue')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss', color='orange')\n",
    "axes[0, 0].set_title('Model Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy', color='blue')\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy', color='orange')\n",
    "axes[0, 1].set_title('Model Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC plot\n",
    "axes[1, 0].plot(history.history['auc'], label='Train AUC', color='blue')\n",
    "axes[1, 0].plot(history.history['val_auc'], label='Val AUC', color='orange')\n",
    "axes[1, 0].set_title('Model AUC')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('AUC')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_true, y_pred_optimal)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1])\n",
    "axes[1, 1].set_title(f'Confusion Matrix (threshold={optimal_threshold:.3f})')\n",
    "axes[1, 1].set_xlabel('Predicted Label')\n",
    "axes[1, 1].set_ylabel('True Label')\n",
    "axes[1, 1].set_xticklabels(['No DR', 'Has DR'])\n",
    "axes[1, 1].set_yticklabels(['No DR', 'Has DR'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test_true, y_test_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC (AUC = {test_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - CNN Baseline')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db34bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 12: FINAL RESULTS SUMMARY\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 12: FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"üß† CNN Baseline Results Summary:\")\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(f\"   Type: Custom CNN (4 conv blocks + 2 dense layers)\")\n",
    "print(f\"   Parameters: {total_params:,}\")\n",
    "print(f\"   Input size: {IMG_SIZE}\")\n",
    "print(f\"   Training time: {training_time/60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\nüéØ Performance Results:\")\n",
    "print(f\"   Test AUC: {test_auc:.4f}\")\n",
    "print(f\"   Default threshold (0.5):\")\n",
    "print(f\"     Accuracy: {test_acc*100:.1f}%\")\n",
    "print(f\"     Precision: {test_prec*100:.1f}%\")\n",
    "print(f\"     Recall: {test_rec*100:.1f}%\")\n",
    "print(f\"     F1: {test_f1*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüè• Optimized threshold ({optimal_threshold:.3f}):\")\n",
    "print(f\"     Accuracy: {optimal_acc*100:.1f}%\")\n",
    "print(f\"     Precision: {optimal_prec*100:.1f}%\")\n",
    "print(f\"     Recall: {optimal_rec*100:.1f}%\")\n",
    "print(f\"     F1: {optimal_f1*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüÜö Model Comparison (Final Ranking):\")\n",
    "all_models = [\n",
    "    ('Random Forest', 0.551, 60.2, 'Traditional ML'),\n",
    "    ('SVM', 0.505, 58.8, 'Traditional ML'),\n",
    "    ('CNN Baseline', test_auc, optimal_rec*100, 'Deep Learning'),\n",
    "    ('VGG16', 0.706, 1.0, 'Transfer Learning'),  # Note: VGG16 recall was broken\n",
    "]\n",
    "\n",
    "# Sort by AUC\n",
    "all_models_sorted = sorted(all_models, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"   {'Rank':<5} {'Model':<15} {'AUC':<8} {'Recall':<8} {'Type':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for i, (name, auc, recall, model_type) in enumerate(all_models_sorted, 1):\n",
    "    print(f\"   {i:<5} {name:<15} {auc:<8.3f} {recall:<8.1f} {model_type:<15}\")\n",
    "\n",
    "print(f\"\\nüéØ Baseline Assessment:\")\n",
    "if test_auc >= 0.65:\n",
    "    print(f\"   ‚úÖ STRONG BASELINE: CNN achieved competitive performance\")\n",
    "    print(f\"   üöÄ Ready for advanced architectures (ResNet50, ensemble)\")\n",
    "elif test_auc >= 0.60:\n",
    "    print(f\"   üëç GOOD BASELINE: CNN shows promise for deep learning approach\")\n",
    "    print(f\"   üìà Transfer learning should provide significant improvement\")\n",
    "elif test_auc >= 0.55:\n",
    "    print(f\"   üìä MODERATE BASELINE: CNN outperforms some traditional ML\")\n",
    "    print(f\"   üîß Consider architecture improvements or more data\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  WEAK BASELINE: Traditional ML may be more suitable\")\n",
    "    print(f\"   üõ†Ô∏è Revise architecture or feature engineering approach\")\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "train_auc = max(history.history['auc'])\n",
    "overfitting = train_auc - test_auc\n",
    "print(f\"   ‚Ä¢ Training AUC: {train_auc:.3f}\")\n",
    "print(f\"   ‚Ä¢ Test AUC: {test_auc:.3f}\")\n",
    "print(f\"   ‚Ä¢ Overfitting: {overfitting:.3f} ({'Low' if overfitting < 0.05 else 'Moderate' if overfitting < 0.1 else 'High'})\")\n",
    "print(f\"   ‚Ä¢ Baseline established for {'transfer learning' if test_auc >= 0.60 else 'traditional ML'} comparison\")\n",
    "\n",
    "print(f\"\\nüìã Recommendations:\")\n",
    "if test_auc >= 0.65:\n",
    "    print(f\"   1. ‚úÖ CNN baseline is strong - proceed with ResNet50\")\n",
    "    print(f\"   2. üéØ Consider ensemble methods (CNN + XGBoost)\")\n",
    "    print(f\"   3. üî¨ Experiment with different architectures\")\n",
    "elif test_auc >= 0.60:\n",
    "    print(f\"   1. üìà ResNet50 transfer learning should significantly improve\")\n",
    "    print(f\"   2. üéõÔ∏è Consider data augmentation improvements\")\n",
    "    print(f\"   3. üîç Compare with XGBoost performance\")\n",
    "else:\n",
    "    print(f\"   1. üõ†Ô∏è Revise CNN architecture (more layers, different activation)\")\n",
    "    print(f\"   2. üìä Focus on XGBoost and feature engineering\")\n",
    "    print(f\"   3. üîÑ Consider larger input size (224x224)\")\n",
    "\n",
    "print(f\"\\nüíæ Model saved as: best_cnn_baseline.h5\")\n",
    "print(f\"üéØ Use this as baseline for comparison with advanced models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e07bc",
   "metadata": {},
   "source": [
    "## üéØ CNN Baseline Experiment Summary\n",
    "\n",
    "### Purpose:\n",
    "Establish a **deep learning baseline** for diabetic retinopathy classification to compare against:\n",
    "1. Traditional ML methods (Random Forest, SVM)\n",
    "2. Advanced deep learning (ResNet50, VGG16)\n",
    "3. Ensemble approaches\n",
    "\n",
    "### Key Features:\n",
    "- **Lightweight architecture**: 4 conv blocks + 2 dense layers\n",
    "- **Fast training**: ~30 minutes vs hours for transfer learning\n",
    "- **Medical-optimized**: Focal loss + class weights for imbalance\n",
    "- **Conservative augmentation**: Appropriate for medical images\n",
    "\n",
    "### Expected Performance:\n",
    "- **Target AUC**: 0.60-0.70 (baseline expectation)\n",
    "- **Training time**: 20-40 minutes\n",
    "- **Memory usage**: Low (smaller than transfer learning)\n",
    "\n",
    "### Success Criteria:\n",
    "- **AUC ‚â• 0.65**: Strong baseline, proceed with advanced models\n",
    "- **AUC 0.60-0.64**: Good baseline, transfer learning should help\n",
    "- **AUC < 0.60**: Consider architecture improvements\n",
    "\n",
    "### Next Steps Based on Results:\n",
    "1. **If CNN baseline performs well**: Compare with ResNet50\n",
    "2. **If CNN baseline struggles**: Focus on XGBoost and feature engineering\n",
    "3. **Either way**: Use as ensemble component\n",
    "\n",
    "### For Production Use:\n",
    "```python\n",
    "# Load the baseline model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Custom objects for focal loss\n",
    "custom_objects = {'focal_loss_fixed': focal_loss(2.0, 0.75)}\n",
    "model = load_model('best_cnn_baseline.h5', custom_objects=custom_objects)\n",
    "\n",
    "# Predict with optimized threshold\n",
    "predictions = model.predict(images)\n",
    "binary_predictions = (predictions > optimal_threshold).astype(int)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
