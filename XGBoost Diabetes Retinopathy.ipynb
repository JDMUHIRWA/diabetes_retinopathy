{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3017cd9",
   "metadata": {},
   "source": [
    "# üöÄ XGBoost for Diabetic Retinopathy Classification\n",
    "\n",
    "## üìã Experiment Overview\n",
    "- **Model**: XGBoost with engineered features\n",
    "- **Task**: Binary classification (No DR vs Has DR)\n",
    "- **Dataset**: Kaggle Diabetic Retinopathy (35,126 images)\n",
    "- **Strategy**: Advanced feature engineering + gradient boosting\n",
    "- **Expected AUC**: 0.65-0.75 (potentially competitive with deep learning)\n",
    "\n",
    "## üéØ Key Features:\n",
    "1. **HOG features** (edge patterns)\n",
    "2. **Color statistics** (RGB, HSV channels)\n",
    "3. **Texture features** (LBP, GLCM)\n",
    "4. **Shape features** (contour analysis)\n",
    "5. **XGBoost hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 1: IMPORT LIBRARIES\n",
    "# ============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine Learning\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Feature Engineering\n",
    "from skimage.feature import hog, local_binary_pattern, graycomatrix, graycoprops\n",
    "from skimage import exposure, filters\n",
    "from scipy import ndimage\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")\n",
    "print(f\"üöÄ XGBoost version: {xgb.__version__}\")\n",
    "print(f\"üî¨ OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68167ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 2: ADVANCED FEATURE EXTRACTION\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: ADVANCED FEATURE EXTRACTION FUNCTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def extract_hog_features(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "    \"\"\"\n",
    "    Extract HOG (Histogram of Oriented Gradients) features\n",
    "    Good for detecting edges and shapes in retinal images\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    features = hog(image, orientations=orientations, \n",
    "                   pixels_per_cell=pixels_per_cell,\n",
    "                   cells_per_block=cells_per_block, \n",
    "                   visualize=False, feature_vector=True)\n",
    "    return features\n",
    "\n",
    "def extract_color_features(image):\n",
    "    \"\"\"\n",
    "    Extract comprehensive color statistics\n",
    "    Important for detecting red lesions and color changes\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # RGB statistics\n",
    "    for channel in range(3):\n",
    "        channel_data = image[:, :, channel]\n",
    "        features.extend([\n",
    "            np.mean(channel_data),\n",
    "            np.std(channel_data),\n",
    "            np.median(channel_data),\n",
    "            np.min(channel_data),\n",
    "            np.max(channel_data)\n",
    "        ])\n",
    "    \n",
    "    # HSV color space (better for medical images)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    for channel in range(3):\n",
    "        channel_data = hsv[:, :, channel]\n",
    "        features.extend([\n",
    "            np.mean(channel_data),\n",
    "            np.std(channel_data)\n",
    "        ])\n",
    "    \n",
    "    # Color ratios (red/green important for DR)\n",
    "    r, g, b = image[:, :, 2], image[:, :, 1], image[:, :, 0]\n",
    "    features.extend([\n",
    "        np.mean(r) / (np.mean(g) + 1e-6),  # Red/Green ratio\n",
    "        np.mean(r) / (np.mean(b) + 1e-6),  # Red/Blue ratio\n",
    "        np.mean(g) / (np.mean(b) + 1e-6)   # Green/Blue ratio\n",
    "    ])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def extract_texture_features(image):\n",
    "    \"\"\"\n",
    "    Extract texture features using LBP and GLCM\n",
    "    Good for detecting surface irregularities\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # Local Binary Pattern\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=n_points + 2, \n",
    "                              range=(0, n_points + 2), density=True)\n",
    "    features.extend(lbp_hist)\n",
    "    \n",
    "    # Gray Level Co-occurrence Matrix (GLCM)\n",
    "    # Resize for computational efficiency\n",
    "    gray_small = cv2.resize(gray, (64, 64))\n",
    "    gray_small = (gray_small / 32).astype(np.uint8)  # Reduce levels for GLCM\n",
    "    \n",
    "    distances = [1, 2]\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    \n",
    "    for distance in distances:\n",
    "        glcm = graycomatrix(gray_small, distances=[distance], \n",
    "                           angles=angles, levels=8, symmetric=True, normed=True)\n",
    "        \n",
    "        # GLCM properties\n",
    "        contrast = graycoprops(glcm, 'contrast').mean()\n",
    "        dissimilarity = graycoprops(glcm, 'dissimilarity').mean()\n",
    "        homogeneity = graycoprops(glcm, 'homogeneity').mean()\n",
    "        energy = graycoprops(glcm, 'energy').mean()\n",
    "        correlation = graycoprops(glcm, 'correlation').mean()\n",
    "        \n",
    "        features.extend([contrast, dissimilarity, homogeneity, energy, correlation])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def extract_shape_features(image):\n",
    "    \"\"\"\n",
    "    Extract shape and morphological features\n",
    "    Good for detecting vessel patterns and lesion shapes\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # Edge detection\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    edge_density = np.sum(edges > 0) / edges.size\n",
    "    features.append(edge_density)\n",
    "    \n",
    "    # Contour analysis\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        # Number of contours\n",
    "        features.append(len(contours))\n",
    "        \n",
    "        # Average contour area\n",
    "        areas = [cv2.contourArea(c) for c in contours if cv2.contourArea(c) > 10]\n",
    "        features.append(np.mean(areas) if areas else 0)\n",
    "        \n",
    "        # Average contour perimeter\n",
    "        perimeters = [cv2.arcLength(c, True) for c in contours if cv2.contourArea(c) > 10]\n",
    "        features.append(np.mean(perimeters) if perimeters else 0)\n",
    "    else:\n",
    "        features.extend([0, 0, 0])  # No contours found\n",
    "    \n",
    "    # Morphological features\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    \n",
    "    # Opening (removes small noise)\n",
    "    opening = cv2.morphologyEx(gray, cv2.MORPH_OPEN, kernel)\n",
    "    opening_diff = np.mean(gray) - np.mean(opening)\n",
    "    features.append(opening_diff)\n",
    "    \n",
    "    # Closing (fills small gaps)\n",
    "    closing = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)\n",
    "    closing_diff = np.mean(closing) - np.mean(gray)\n",
    "    features.append(closing_diff)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def extract_all_features(image_path):\n",
    "    \"\"\"\n",
    "    Extract all features from an image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            return None\n",
    "        \n",
    "        # Resize to standard size for consistency\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        \n",
    "        # Extract all feature types\n",
    "        hog_features = extract_hog_features(image)\n",
    "        color_features = extract_color_features(image)\n",
    "        texture_features = extract_texture_features(image)\n",
    "        shape_features = extract_shape_features(image)\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = np.concatenate([\n",
    "            hog_features,\n",
    "            color_features,\n",
    "            texture_features,\n",
    "            shape_features\n",
    "        ])\n",
    "        \n",
    "        return all_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Feature extraction functions defined!\")\n",
    "print(\"   üìä HOG: Edge and shape patterns\")\n",
    "print(\"   üé® Color: RGB, HSV, and ratios\")\n",
    "print(\"   üîç Texture: LBP and GLCM properties\")\n",
    "print(\"   üìê Shape: Contours and morphology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f9ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 3: DATA LOADING & SETUP\n",
    "# ============================================\n",
    "# For Kaggle - adjust paths as needed\n",
    "KAGGLE_INPUT_PATH = '/kaggle/input/diabetic-retinopathy-resized'\n",
    "LOCAL_PATH = '/Users/muhirwa/Desktop/projects/diabetes_retinopathy/data'\n",
    "\n",
    "# Try Kaggle path first, fallback to local\n",
    "if os.path.exists(KAGGLE_INPUT_PATH):\n",
    "    DATA_PATH = KAGGLE_INPUT_PATH\n",
    "    print(\"üåê Using Kaggle dataset path\")\n",
    "elif os.path.exists(LOCAL_PATH):\n",
    "    DATA_PATH = LOCAL_PATH\n",
    "    print(\"üíª Using local dataset path\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset not found! Please check paths.\")\n",
    "    DATA_PATH = None\n",
    "\n",
    "print(f\"üìÅ Data path: {DATA_PATH}\")\n",
    "\n",
    "# Load labels\n",
    "if DATA_PATH:\n",
    "    labels_path = os.path.join(DATA_PATH, 'trainLabels.csv')\n",
    "    if os.path.exists(labels_path):\n",
    "        labels_df = pd.read_csv(labels_path)\n",
    "        print(f\"üìä Labels loaded! Total images: {len(labels_df)}\")\n",
    "        \n",
    "        # Convert to binary classification\n",
    "        labels_df['binary_label'] = (labels_df['level'] > 0).astype(int)\n",
    "        \n",
    "        print(\"\\nüéØ Binary classification:\")\n",
    "        print(labels_df['binary_label'].value_counts())\n",
    "        print(f\"  No DR (0): {(labels_df['binary_label']==0).sum()} images ({(labels_df['binary_label']==0).sum()/len(labels_df)*100:.1f}%)\")\n",
    "        print(f\"  Has DR (1): {(labels_df['binary_label']==1).sum()} images ({(labels_df['binary_label']==1).sum()/len(labels_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Add file paths\n",
    "        image_dir = os.path.join(DATA_PATH, 'resized_train_cropped', 'resized_train_cropped')\n",
    "        labels_df['filepath'] = labels_df['image'].apply(lambda x: os.path.join(image_dir, f\"{x}.jpeg\"))\n",
    "        \n",
    "        # For faster experimentation, use a subset (uncomment if needed)\n",
    "        # labels_df = labels_df.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "        # print(f\"Using subset of {len(labels_df)} images for faster processing\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Labels file not found at {labels_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 4: FEATURE EXTRACTION\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: EXTRACTING FEATURES FROM ALL IMAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"üìä Extracting features from {len(labels_df)} images...\")\n",
    "print(\"‚è≥ This may take several minutes...\")\n",
    "\n",
    "start_time = time.time()\n",
    "features_list = []\n",
    "valid_indices = []\n",
    "\n",
    "# Extract features with progress bar\n",
    "for idx, row in tqdm(labels_df.iterrows(), total=len(labels_df), desc=\"Extracting features\"):\n",
    "    features = extract_all_features(row['filepath'])\n",
    "    \n",
    "    if features is not None:\n",
    "        features_list.append(features)\n",
    "        valid_indices.append(idx)\n",
    "    \n",
    "    # Progress update every 1000 images\n",
    "    if (idx + 1) % 1000 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Processed {idx + 1}/{len(labels_df)} images ({elapsed:.1f}s)\")\n",
    "\n",
    "# Convert to numpy array\n",
    "if features_list:\n",
    "    X = np.array(features_list)\n",
    "    y = labels_df.loc[valid_indices, 'binary_label'].values\n",
    "    \n",
    "    extraction_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Feature extraction completed!\")\n",
    "    print(f\"   Time taken: {extraction_time:.2f} seconds ({extraction_time/60:.2f} minutes)\")\n",
    "    print(f\"   Valid images: {len(features_list)}/{len(labels_df)}\")\n",
    "    print(f\"   Feature shape: {X.shape}\")\n",
    "    print(f\"   Total features per image: {X.shape[1]}\")\n",
    "    \n",
    "    # Feature breakdown\n",
    "    hog_size = len(extract_hog_features(cv2.imread(labels_df.iloc[0]['filepath'])))\n",
    "    color_size = len(extract_color_features(cv2.imread(labels_df.iloc[0]['filepath'])))\n",
    "    print(f\"\\nüìä Feature breakdown:\")\n",
    "    print(f\"   HOG features: {hog_size}\")\n",
    "    print(f\"   Color features: {color_size}\")\n",
    "    print(f\"   Texture features: ~26 (LBP + GLCM)\")\n",
    "    print(f\"   Shape features: ~6\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No features extracted! Check image paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdb2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 5: DATA PREPROCESSING\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"üìä Data splits:\")\n",
    "print(f\"   Train: {X_train.shape[0]} samples\")\n",
    "print(f\"   Val:   {X_val.shape[0]} samples\")\n",
    "print(f\"   Test:  {X_test.shape[0]} samples\")\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nüìä Class distribution:\")\n",
    "for name, y_split in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
    "    class_counts = np.bincount(y_split)\n",
    "    print(f\"   {name:5s}: No DR = {class_counts[0]:4d} ({class_counts[0]/len(y_split)*100:.1f}%), Has DR = {class_counts[1]:4d} ({class_counts[1]/len(y_split)*100:.1f}%)\")\n",
    "\n",
    "# Feature scaling (important for XGBoost)\n",
    "print(f\"\\nüîß Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Features scaled (mean=0, std=1)\")\n",
    "print(f\"   Train features: mean={X_train_scaled.mean():.3f}, std={X_train_scaled.std():.3f}\")\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "scale_pos_weight = class_weights[1] / class_weights[0]\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  Class weights:\")\n",
    "print(f\"   No DR (0): {class_weights[0]:.3f}\")\n",
    "print(f\"   Has DR (1): {class_weights[1]:.3f}\")\n",
    "print(f\"   Scale pos weight: {scale_pos_weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 6: XGBOOST MODEL TRAINING\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: XGBOOST MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# XGBoost parameters optimized for medical data\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'scale_pos_weight': scale_pos_weight,  # Handle class imbalance\n",
    "    'max_depth': 6,                        # Prevent overfitting\n",
    "    'learning_rate': 0.1,                  # Conservative learning\n",
    "    'n_estimators': 500,                   # Sufficient trees\n",
    "    'subsample': 0.8,                      # Row sampling\n",
    "    'colsample_bytree': 0.8,              # Feature sampling\n",
    "    'min_child_weight': 3,                 # Regularization\n",
    "    'gamma': 0.1,                          # Minimum split loss\n",
    "    'reg_alpha': 0.1,                      # L1 regularization\n",
    "    'reg_lambda': 1.0,                     # L2 regularization\n",
    "    'random_state': 42,\n",
    "    'verbosity': 1\n",
    "}\n",
    "\n",
    "print(f\"üöÄ Training XGBoost with optimized parameters...\")\n",
    "print(f\"   Trees: {xgb_params['n_estimators']}\")\n",
    "print(f\"   Max depth: {xgb_params['max_depth']}\")\n",
    "print(f\"   Learning rate: {xgb_params['learning_rate']}\")\n",
    "print(f\"   Scale pos weight: {scale_pos_weight:.3f}\")\n",
    "\n",
    "# Create and train model\n",
    "model = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train with early stopping\n",
    "model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_train_scaled, y_train), (X_val_scaled, y_val)],\n",
    "    eval_names=['train', 'val'],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=50  # Print every 50 rounds\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"   Best iteration: {model.best_iteration}\")\n",
    "print(f\"   Best score: {model.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb98818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 7: MODEL EVALUATION\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 7: MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_proba = model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_val_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "y_test_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "test_rec = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "test_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "print(f\"üìä Test Set Results (threshold=0.5):\")\n",
    "print(f\"   Accuracy:  {test_acc*100:.2f}%\")\n",
    "print(f\"   Precision: {test_prec*100:.2f}%\")\n",
    "print(f\"   Recall:    {test_rec*100:.2f}%\")\n",
    "print(f\"   F1 Score:  {test_f1*100:.2f}%\")\n",
    "print(f\"   ROC AUC:   {test_auc:.4f}\")\n",
    "\n",
    "# Validation performance\n",
    "val_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "\n",
    "print(f\"\\nüìà AUC Scores:\")\n",
    "print(f\"   Train AUC: {train_auc:.4f}\")\n",
    "print(f\"   Val AUC:   {val_auc:.4f}\")\n",
    "print(f\"   Test AUC:  {test_auc:.4f}\")\n",
    "print(f\"   Overfitting: {train_auc - test_auc:.4f}\")\n",
    "\n",
    "# Comparison with other models\n",
    "print(f\"\\nüÜö Comparison with other models:\")\n",
    "print(f\"   Random Forest: AUC 0.551\")\n",
    "print(f\"   SVM:          AUC 0.505\")\n",
    "print(f\"   VGG16:        AUC 0.706\")\n",
    "print(f\"   XGBoost:      AUC {test_auc:.3f}\")\n",
    "\n",
    "if test_auc > 0.706:\n",
    "    print(\"üéâ XGBoost outperformed VGG16!\")\n",
    "elif test_auc > 0.65:\n",
    "    print(\"üëç XGBoost shows competitive performance!\")\n",
    "else:\n",
    "    print(\"üìà XGBoost needs improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be51fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 8: THRESHOLD OPTIMIZATION\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 8: THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find optimal threshold for medical use\n",
    "precision_curve, recall_curve, thresholds = precision_recall_curve(y_test, y_test_pred_proba)\n",
    "\n",
    "# Target 70% recall for medical screening\n",
    "target_recall = 0.70\n",
    "recall_diffs = np.abs(recall_curve - target_recall)\n",
    "optimal_idx = np.argmin(recall_diffs)\n",
    "optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
    "\n",
    "print(f\"üéØ Threshold optimization:\")\n",
    "print(f\"   Target recall: {target_recall*100:.0f}%\")\n",
    "print(f\"   Optimal threshold: {optimal_threshold:.3f}\")\n",
    "\n",
    "# Test different thresholds\n",
    "print(f\"\\nüìä Performance at different thresholds:\")\n",
    "print(f\"{'Threshold':<10} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for thresh in [0.1, 0.2, 0.3, 0.4, 0.5, optimal_threshold]:\n",
    "    y_pred_thresh = (y_test_pred_proba > thresh).astype(int)\n",
    "    acc = accuracy_score(y_test, y_pred_thresh)\n",
    "    prec = precision_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    \n",
    "    print(f\"{thresh:<10.2f} {acc*100:<10.1f} {prec*100:<10.1f} {rec*100:<10.1f} {f1*100:<10.1f}\")\n",
    "\n",
    "# Optimal threshold results\n",
    "y_pred_optimal = (y_test_pred_proba > optimal_threshold).astype(int)\n",
    "optimal_acc = accuracy_score(y_test, y_pred_optimal)\n",
    "optimal_prec = precision_score(y_test, y_pred_optimal, zero_division=0)\n",
    "optimal_rec = recall_score(y_test, y_pred_optimal, zero_division=0)\n",
    "optimal_f1 = f1_score(y_test, y_pred_optimal, zero_division=0)\n",
    "\n",
    "print(f\"\\nüè• Optimized for medical use (threshold={optimal_threshold:.3f}):\")\n",
    "print(f\"   Accuracy:  {optimal_acc*100:.2f}%\")\n",
    "print(f\"   Precision: {optimal_prec*100:.2f}%\")\n",
    "print(f\"   Recall:    {optimal_rec*100:.2f}%\")\n",
    "print(f\"   F1 Score:  {optimal_f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc1023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 9: FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 9: FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "feature_names = [f'Feature_{i}' for i in range(len(feature_importance))]\n",
    "\n",
    "# Create feature importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"üìä Top 15 most important features:\")\n",
    "print(importance_df.head(15).to_string(index=False))\n",
    "\n",
    "# Feature type analysis (approximate)\n",
    "hog_size = len(extract_hog_features(cv2.imread(labels_df.iloc[0]['filepath'])))\n",
    "color_size = len(extract_color_features(cv2.imread(labels_df.iloc[0]['filepath'])))\n",
    "texture_start = hog_size + color_size\n",
    "\n",
    "# Calculate importance by feature type\n",
    "hog_importance = feature_importance[:hog_size].sum()\n",
    "color_importance = feature_importance[hog_size:hog_size+color_size].sum()\n",
    "texture_importance = feature_importance[texture_start:texture_start+26].sum()\n",
    "shape_importance = feature_importance[texture_start+26:].sum()\n",
    "\n",
    "print(f\"\\nüéØ Feature type importance:\")\n",
    "print(f\"   HOG (edges):     {hog_importance:.3f} ({hog_importance/feature_importance.sum()*100:.1f}%)\")\n",
    "print(f\"   Color:           {color_importance:.3f} ({color_importance/feature_importance.sum()*100:.1f}%)\")\n",
    "print(f\"   Texture:         {texture_importance:.3f} ({texture_importance/feature_importance.sum()*100:.1f}%)\")\n",
    "print(f\"   Shape:           {shape_importance:.3f} ({shape_importance/feature_importance.sum()*100:.1f}%)\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "top_features = importance_df.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "plt.yticks(range(len(top_features)), [f'F_{i}' for i in range(len(top_features))])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Feature type pie chart\n",
    "plt.subplot(2, 2, 2)\n",
    "feature_types = ['HOG', 'Color', 'Texture', 'Shape']\n",
    "importances = [hog_importance, color_importance, texture_importance, shape_importance]\n",
    "plt.pie(importances, labels=feature_types, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Feature Type Importance')\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.subplot(2, 2, 3)\n",
    "cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix (threshold={optimal_threshold:.3f})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(2, 2, 4)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_pred_proba)\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC (AUC = {test_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - XGBoost')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a8a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 10: FINAL RESULTS SUMMARY\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 10: FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"üöÄ XGBoost Results Summary:\")\n",
    "print(f\"\\nüìä Model Configuration:\")\n",
    "print(f\"   Algorithm: XGBoost Classifier\")\n",
    "print(f\"   Features: {X.shape[1]} (HOG + Color + Texture + Shape)\")\n",
    "print(f\"   Trees: {model.best_iteration}\")\n",
    "print(f\"   Training time: {training_time/60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\nüéØ Performance:\")\n",
    "print(f\"   Test AUC: {test_auc:.4f}\")\n",
    "print(f\"   Default threshold (0.5):\")\n",
    "print(f\"     Accuracy: {test_acc*100:.1f}%\")\n",
    "print(f\"     Precision: {test_prec*100:.1f}%\")\n",
    "print(f\"     Recall: {test_rec*100:.1f}%\")\n",
    "print(f\"     F1: {test_f1*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüè• Medical threshold ({optimal_threshold:.3f}):\")\n",
    "print(f\"     Accuracy: {optimal_acc*100:.1f}%\")\n",
    "print(f\"     Precision: {optimal_prec*100:.1f}%\")\n",
    "print(f\"     Recall: {optimal_rec*100:.1f}%\")\n",
    "print(f\"     F1: {optimal_f1*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüÜö Model Comparison:\")\n",
    "models_results = [\n",
    "    ('Random Forest', 0.551, 60.2),\n",
    "    ('SVM', 0.505, 58.8),\n",
    "    ('VGG16', 0.706, 1.0),\n",
    "    ('XGBoost', test_auc, optimal_rec*100)\n",
    "]\n",
    "\n",
    "for model_name, auc, recall in models_results:\n",
    "    print(f\"   {model_name:<15}: AUC {auc:.3f}, Recall {recall:.1f}%\")\n",
    "\n",
    "# Determine ranking\n",
    "best_auc = max(models_results, key=lambda x: x[1])\n",
    "best_recall = max(models_results, key=lambda x: x[2])\n",
    "\n",
    "print(f\"\\nüèÜ Rankings:\")\n",
    "print(f\"   Best AUC: {best_auc[0]} ({best_auc[1]:.3f})\")\n",
    "print(f\"   Best Recall: {best_recall[0]} ({best_recall[2]:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Key Insights:\")\n",
    "print(f\"   ‚Ä¢ Most important features: {feature_types[np.argmax(importances)]} ({max(importances)/sum(importances)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Feature engineering effectiveness: {'High' if test_auc > 0.65 else 'Moderate'}\")\n",
    "print(f\"   ‚Ä¢ Overfitting: {'Low' if abs(train_auc - test_auc) < 0.05 else 'Moderate'}\")\n",
    "\n",
    "if test_auc >= 0.70:\n",
    "    print(f\"\\n‚úÖ EXCELLENT: XGBoost achieved competitive deep learning performance!\")\n",
    "elif test_auc >= 0.65:\n",
    "    print(f\"\\nüëç GOOD: XGBoost shows strong traditional ML performance!\")\n",
    "else:\n",
    "    print(f\"\\nüìà ROOM FOR IMPROVEMENT: Consider feature engineering or hyperparameter tuning\")\n",
    "\n",
    "print(f\"\\nüíæ Next steps:\")\n",
    "print(f\"   ‚Ä¢ If AUC ‚â• 0.70: XGBoost is competitive with deep learning\")\n",
    "print(f\"   ‚Ä¢ If AUC < 0.70: Try CNN Baseline or ensemble methods\")\n",
    "print(f\"   ‚Ä¢ Consider combining XGBoost with ResNet50 for ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c876ba7",
   "metadata": {},
   "source": [
    "## üéØ XGBoost Experiment Summary\n",
    "\n",
    "### Key Achievements:\n",
    "1. **Comprehensive feature engineering**: HOG, color, texture, and shape features\n",
    "2. **Advanced XGBoost tuning**: Scale pos weight, regularization, early stopping\n",
    "3. **Medical threshold optimization**: Prioritizing recall for clinical use\n",
    "4. **Feature importance analysis**: Understanding which features matter most\n",
    "\n",
    "### Strengths of XGBoost:\n",
    "- **Interpretable**: Can analyze which features are most important\n",
    "- **Fast training**: Much faster than deep learning\n",
    "- **Robust**: Less prone to overfitting with proper regularization\n",
    "- **Feature engineering**: Leverages domain knowledge about medical images\n",
    "\n",
    "### Expected Performance:\n",
    "- **Target AUC**: 0.65-0.75 (competitive with deep learning)\n",
    "- **Recall**: 60-80% with optimized threshold\n",
    "- **Training time**: 5-15 minutes (vs hours for deep learning)\n",
    "\n",
    "### For Production Use:\n",
    "```python\n",
    "# Save the model and scaler\n",
    "import joblib\n",
    "joblib.dump(model, 'xgboost_dr_model.pkl')\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "\n",
    "# Load and predict\n",
    "model = joblib.load('xgboost_dr_model.pkl')\n",
    "scaler = joblib.load('feature_scaler.pkl')\n",
    "\n",
    "# Extract features and predict\n",
    "features = extract_all_features(image_path)\n",
    "features_scaled = scaler.transform(features.reshape(1, -1))\n",
    "probability = model.predict_proba(features_scaled)[0, 1]\n",
    "prediction = probability > optimal_threshold\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
